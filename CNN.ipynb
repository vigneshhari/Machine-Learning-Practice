{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "   Taught By Vignesh Hari , Using MNIST Dataset\n",
    "   \n",
    "## Requirements\n",
    "We'll be using \n",
    "Tensorflow 1.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "Downloading and Using the MNIST Dataset \n",
    "The MNIST Dataset includes images of handwritten digits, they are 28 x 28 in size and have only one channel ( ie Black and White )  \n",
    "One Hot Encoding Means that the y value is stored as a position of a list rather than a value   \n",
    "eg : [0,0,1] instead of 2  ||  [0,1,0] instead of 1 || and so on ..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "The data to be used in the Tensorflow Session is taken as a Tensorflow Placeholder , Since the images are 28 x 28 in size , We use (28 * 28) sized vectors to store the unrolled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inp = tf.placeholder(tf.float32, [None, 784]) # 28 * 28 Input nodes\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 10 input classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "To use a Convolutional Neural Net we have to Convolute + Pool + Fully Connect ( Will be Explained later )\n",
    "\n",
    "Convoluting means creating feature maps from the inputs by creating small frames and running it through the data, The Data has to be converted into its original dimention for this , hence we reshape the data back to 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.reshape(x_inp, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_conv1  = tf.Variable(tf.random_normal([5,5,1,32])) \n",
    "weight_conv2  = tf.Variable(tf.random_normal([5,5,32,64]))\n",
    "weight_fc     = tf.Variable(tf.random_normal([7*7*64,784]))\n",
    "weight_out    = tf.Variable(tf.random_normal([784,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_conv1  = tf.Variable(tf.random_normal([32])) \n",
    "bias_conv2  = tf.Variable(tf.random_normal([64]))\n",
    "bias_fc     = tf.Variable(tf.random_normal([784]))\n",
    "bias_out    = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1     = tf.add(tf.nn.conv2d(x , weight_conv1 , strides=[1,1,1,1] , padding=\"SAME\") , bias_conv1) \n",
    "max_pool1 = tf.nn.max_pool(conv1 , ksize=[1,2,2,1] , strides=[1,2,2,1] , padding=\"SAME\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2     = tf.add(tf.nn.conv2d(max_pool1 , weight_conv2 , strides=[1,1,1,1] , padding=\"SAME\") , bias_conv2)\n",
    "max_pool2 = tf.nn.max_pool(conv2 , ksize=[1,2,2,1] , strides=[1,2,2,1] , padding=\"SAME\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = tf.reshape(max_pool2 , [-1 , 7*7*64 ])\n",
    "fc = tf.nn.relu( tf.add(tf.matmul(fc , weight_fc) , bias_fc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = tf.matmul(fc , weight_out) + bias_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y) )\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initialise tensoflow session\n",
    "'''\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'completed out of', 1, 'loss:', 1781413.0296020508)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hm_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x_inp: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.9274)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',sess.run(accuracy , feed_dict={x_inp:mnist.test.images, y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
