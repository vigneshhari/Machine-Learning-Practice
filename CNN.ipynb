{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "   Taught By Vignesh Hari , Using MNIST Dataset\n",
    "   \n",
    "## Requirements\n",
    "We'll be using \n",
    "Tensorflow 1.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "Downloading and Using the MNIST Dataset \n",
    "The MNIST Dataset includes images of handwritten digits, they are 28 x 28 in size and have only one channel ( ie Black and White )  \n",
    "One Hot Encoding Means that the y value is stored as a position of a list rather than a value   \n",
    "eg : [0,0,1] instead of 2  ||  [0,1,0] instead of 1 || and so on ..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "The data to be used in the Tensorflow Session is taken as a Tensorflow Placeholder , Since the images are 28 x 28 in size , We use (28 * 28) sized vectors to store the unrolled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_inp = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 10 input classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "To use a Convolutional Neural Net we have to Convolute + Pool + Fully Connect ( Will be Explained later )\n",
    "\n",
    "Convoluting means creating feature maps from the inputs by creating small frames and running it through the data, The Data has to be converted into its original dimention for this , hence we reshape the data back to 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.reshape(x_inp, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "Our Convolutions are 5x5 images , we are extracting 32 such convolutions in the first layer , So the first layer has 32 nodes its size is (5x5x1x32)  \n",
    "  \n",
    "The Second layer has 64 convolutions of each 5x5 and the input is from the previous layer which makes its size (5x5x32x64)\n",
    "\n",
    "The use pooling to summarise the features made in convolution , the pooling scales down the image by the pool size , we are using 2x2 pools twice so the image gets reduced by 4 times , the 28x28 image becomes a 7x7 feature set , so now the output of the second layer will have an unrolled size of 7*7*64 , and the no of nodes in the fully connected layer can be decided by you , but for now we'll use 784 (28x28)\n",
    "\n",
    "The output layer finally has 10 output classes and 784 input nodes , so the size of the weight is (784,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_conv1  = tf.Variable(tf.random_normal([5,5,1,32])) \n",
    "weight_conv2  = tf.Variable(tf.random_normal([5,5,32,64]))\n",
    "weight_fc     = tf.Variable(tf.random_normal([7*7*64,784]))\n",
    "weight_out    = tf.Variable(tf.random_normal([784,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "\n",
    "Biases are used to make sure a neuron shoots a value even when it dosent have a value, the size of the bias is the no of nodes in that layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_conv1  = tf.Variable(tf.random_normal([32])) \n",
    "bias_conv2  = tf.Variable(tf.random_normal([64]))\n",
    "bias_fc     = tf.Variable(tf.random_normal([784]))\n",
    "bias_out    = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1\n",
    "\n",
    "In Layer one we take the initial 28x28 image and convolute it with 1x1 strides , then pool it with max_pool with sizes of 2x2 , skipping over 2x2 generating an image of 14x14  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1     = tf.add(tf.nn.conv2d(x , weight_conv1 , strides=[1,1,1,1] , padding=\"SAME\") , bias_conv1) \n",
    "max_pool1 = tf.nn.max_pool(conv1 , ksize=[1,2,2,1] , strides=[1,2,2,1] , padding=\"SAME\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2\n",
    "\n",
    "In Layer two we take the layer one 14x14 image and convolute it with 1x1 strides , then pool it with max_pool with sizes of 2x2 , skipping over 2x2 generating an image of 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2     = tf.add(tf.nn.conv2d(max_pool1 , weight_conv2 , strides=[1,1,1,1] , padding=\"SAME\") , bias_conv2)\n",
    "max_pool2 = tf.nn.max_pool(conv2 , ksize=[1,2,2,1] , strides=[1,2,2,1] , padding=\"SAME\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3\n",
    "\n",
    "In Layer we unroll the data from the convolution and then multiply the weights and use the RElU Activation function to find the output value of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = tf.reshape(max_pool2 , [-1 , 7*7*64 ])\n",
    "fc = tf.nn.relu( tf.add(tf.matmul(fc , weight_fc) , bias_fc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4\n",
    "\n",
    "In the Final Layer we multiply the final weights and get the final output ( prediction value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = tf.matmul(fc , weight_out) + bias_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (cost) and Optimisation \n",
    "\n",
    "Finally we calculate the loss function or the cost function using softmax cross entropy   \n",
    "\n",
    "$$( -1 * ((y * log(y)) + ((1-y) * log(1-y))   )$$\n",
    "\n",
    "And we train the model using the Adam Optimiser built into tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y) )\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy \n",
    "\n",
    "We calculate Accuracy by calculating the mean of the correct values in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, 'float')) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Initialising a TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the Dataset (MNIST)\n",
    "\n",
    "using a batch size of 128 and training for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Epoch# :  0  : Epochs Left :  1  : loss :  1728728.2463989258\n",
      "Completed Epoch# :  1  : Epochs Left :  0  : loss :  420482.5573348999\n"
     ]
    }
   ],
   "source": [
    "hm_epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x_inp: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c\n",
    "\n",
    "    print 'Completed Epoch# : ', epoch, ' : Epochs Left : ',hm_epochs-epoch - 1,' : loss : ',epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy For Training is  94.53 %\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy For Training is ' ,sess.run(accuracy , feed_dict={x_inp:mnist.test.images, y:mnist.test.labels}) , \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
